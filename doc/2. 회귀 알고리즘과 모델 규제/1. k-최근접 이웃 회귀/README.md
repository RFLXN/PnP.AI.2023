# 2-1. k-최근접 이웃 회귀

## 지도 학습 알고리즘의 분류

* 분류: 샘플을 정해진 몇개의 클래스로 분류하는 알고리즘
* 회귀(regression): 특정 클래스로 나누지 않고 임의의 수를 예측하는 알고리즘

## k-최근접 이웃 알고리즘

* k-최근접 이웃 분류: 예측하려는 샘플에서 가장 가까운 샘플 k개(이웃)를 선택한 후, 이웃들 중 더 많은 클래스로 샘플을 예측
* k-최근접 이웃 회귀: 분류와 똑같이, 예측하려는 샘플에서 이웃을 선택 후, 이웃들의 평균값으로 샘플값을 예측함

## k-최근접 이웃 회귀 실행해보기

우선 데이터를 준비한다.
```python
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

위의 데이터를 k-최근접 이웃 회귀를 위해 가공한다.

```python
# 테스트 모델과 학습 모델 분리
input_train, input_test, target_train, target_test = train_test_split(perch_length, perch_weight, random_state=42)

# sklearn의 fit에는 1차원이 아닌 2차원 배열이 필요하므로 reshape 메소드를 이용해 2차원으로 변경
input_train = input_train.reshape(-1, 1)
input_test = input_test.reshape(-1, 1)
```

k-최근접 이웃 분류에서는 KNeighborsClassifier 클래스를 이용했지만, k-최근접 이웃 회귀에서는 KNeighborsRegressor 클래스를 이용한다.    
해당 클래스를 이용해서 k-최근접 이웃 분류를 학습시키고, 테스트를 진행한다.

```python
kn = KNeighborsRegressor()
kn.fit(input_train, target_train)

kn.score(input_test, target_test)
# output: 0.992809406101064
```

점수가 100이 아닌 100에 가까운 점수가 나왔다. 분류와 다르게 회귀는 특정 클래스가 아닌 임의의 값으로 예측하기 때문에 100% 정확한 값을 예측하기 매우 힘들다.    
회귀 같은 경우에는 점수를 결정계수(coefficient of determination, R<sup>2</sup>)를 이용하여 평가한다.    
결정계수 계산 방법은 다음과 같다.    

$$R^2 = 1 - {({타겟} - {예측})^2의 합 \over ({타겟} - {평균})^2의 합}$$

## 과대적합과 과소적합

이번에는 테스트 세트 대신 훈련 세트로 점수를 내 보자.
```python
kn.score(input_train, target_train)
# output: 0.9698823289099254
```

오히려 테스트 세트에 비해서 점수가 낮게 나오는 것을 볼 수 있다.    
이것과 비슷하게, 테스트 세트에서 점수가 높게 나왓는데 훈련 세트에서의 점수가 매우 낮거나, 
테스트 세트와 훈련 세트의 점수가 모두 낮은 경우를 underfitting(과소적합)이라고 한다.    
반대로 훈련 세트에서 점수가 높았는데, 테스트 세트에서 점수가 낮게 나오는 것을 overfitting(과대적합)이라고 한다.

과소적합을 해결하는 방법으로는 모델을 더 복잡하게 만들어서 해결할 수 있다.    
k-최근접 이웃 모델에서는 이웃 수를 줄여서 모델을 복잡하게 만들 수 있다.

```python
few_kn = KNeighborsRegressor(n_neighbors=3)
few_kn.fit(input_train, target_train)

few_kn.score(input_test, target_test)
# output: 0.9746459963987609

few_score_input = few_kn.score(input_train, target_train)
# output: 0.9804899950518966
```

이전에 비해 테스트 세트와 훈련 세트의 점수 차이가 줄어들었기 때문에 과소적합을 해결했다고 볼 수 있다.    
참고로, 과대적합을 해결하려면 모델을 더 단순화 시키면 된다. (k-최근접 이웃 모델에서는 이웃 수를 늘이면 된다.)


</br></br></br></br>

[다음 (2-2. 선형 회귀) ->](https://github.com/RFLXN/PnP.AI.2023/tree/main/doc/2.%20%ED%9A%8C%EA%B7%80%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EA%B3%BC%20%EB%AA%A8%EB%8D%B8%20%EA%B7%9C%EC%A0%9C/2.%20%EC%84%A0%ED%98%95%20%ED%9A%8C%EA%B7%80)